{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasibehmohammadi/Thesis/blob/main/gan_gru_d6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R6VRrU5EuWl",
        "outputId": "87ac5ca1-b999-46ff-a792-84cebca65fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 توزیع کلاس‌های اصلی در داده‌ها: (array([0, 1]), array([2978, 7022]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Discriminator Loss: 0.6914, Generator Loss: 0.6932\n",
            "Epoch 100: Discriminator Loss: 0.6919, Generator Loss: 0.6795\n",
            "Epoch 200: Discriminator Loss: 0.6933, Generator Loss: 0.6678\n",
            "Epoch 300: Discriminator Loss: 0.6970, Generator Loss: 0.6572\n",
            "Epoch 400: Discriminator Loss: 0.7031, Generator Loss: 0.6475\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Reshape, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. بارگذاری دیتاست d6\n",
        "file_path = \"/content/d6.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2. پردازش داده‌ها\n",
        "target_column = \"F\"\n",
        "features = df.drop(columns=[target_column])\n",
        "target = df[target_column]\n",
        "\n",
        "# بررسی توزیع کلاس‌ها\n",
        "print(\"🔹 توزیع کلاس‌های اصلی در داده‌ها:\", np.unique(target, return_counts=True))\n",
        "\n",
        "# 3. نرمال‌سازی داده‌ها\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# 4. تقسیم داده‌ها به آموزش و تست\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. تغییر شکل داده‌ها برای ورودی GRU\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# 6. تعریف Generator بهبودیافته\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Input(shape=(1, X_train.shape[2])),\n",
        "        GRU(512, return_sequences=True),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        GRU(256, return_sequences=True),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        GRU(128, return_sequences=False),\n",
        "        Dense(X_train.shape[2]),\n",
        "        Reshape((1, X_train.shape[2]))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 7. تعریف Discriminator بهبودیافته\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Input(shape=(1, X_train.shape[2])),\n",
        "        GRU(512, return_sequences=True),\n",
        "        Dropout(0.4),\n",
        "        BatchNormalization(),\n",
        "        GRU(256, return_sequences=False),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 8. تعریف مدل GAN\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential([generator, discriminator])\n",
        "    return model\n",
        "\n",
        "# 9. کامپایل مدل‌ها\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.00005, 0.5), metrics=[\"accuracy\"])\n",
        "\n",
        "generator = build_generator()\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.00005, 0.5))\n",
        "\n",
        "# 10. آموزش مدل GAN\n",
        "epochs = 3000\n",
        "batch_size = 512\n",
        "half_batch = batch_size // 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # ایجاد داده‌های جعلی\n",
        "    noise = np.random.normal(0, 1, (half_batch, 1, X_train.shape[2]))\n",
        "    generated_data = generator.predict(noise, verbose=0)\n",
        "\n",
        "    # انتخاب داده‌های واقعی تصادفی\n",
        "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "    real_data = X_train[idx]\n",
        "\n",
        "    # برچسب‌های واقعی و جعلی (Label Smoothing)\n",
        "    real_labels = np.random.uniform(0.9, 1.0, (half_batch, 1))\n",
        "    fake_labels = np.random.uniform(0.0, 0.1, (half_batch, 1))\n",
        "\n",
        "    # ترکیب داده‌های واقعی و جعلی\n",
        "    X_combined = np.concatenate((real_data, generated_data))\n",
        "    y_combined = np.concatenate((real_labels, fake_labels))\n",
        "\n",
        "    # آموزش Discriminator در هر 2 epoch\n",
        "    if epoch % 2 == 0:\n",
        "        d_loss = discriminator.train_on_batch(X_combined, y_combined)\n",
        "\n",
        "    # آموزش Generator برای فریب دادن Discriminator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 1, X_train.shape[2]))\n",
        "    y_mislabeled = np.ones((batch_size, 1))\n",
        "\n",
        "    g_loss = gan.train_on_batch(noise, y_mislabeled)\n",
        "\n",
        "    d_loss = float(d_loss[0]) if isinstance(d_loss, list) else float(d_loss)\n",
        "    g_loss = float(g_loss[0]) if isinstance(g_loss, list) else float(g_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Discriminator Loss: {d_loss:.4f}, Generator Loss: {g_loss:.4f}\")\n",
        "\n",
        "# 11. تولید داده‌های جدید برای کلاس اقلیت\n",
        "minority_class = 0 if np.bincount(y_train)[0] < np.bincount(y_train)[1] else 1\n",
        "num_samples_to_generate = abs(np.bincount(y_train)[0] - np.bincount(y_train)[1])\n",
        "\n",
        "noise = np.random.normal(0, 1, (num_samples_to_generate, 1, X_train.shape[2]))\n",
        "generated_samples = generator.predict(noise, verbose=0)\n",
        "\n",
        "# ایجاد لیبل‌های جدید برای داده‌های تولید شده\n",
        "synthetic_labels = np.full((num_samples_to_generate,), minority_class)\n",
        "\n",
        "# 12. ترکیب داده‌های واقعی و داده‌های تولیدشده برای متعادل‌سازی کلاس‌ها\n",
        "X_balanced = np.vstack((features_scaled, generated_samples.squeeze()))\n",
        "y_balanced = np.concatenate((target, synthetic_labels))\n",
        "\n",
        "# 13. ذخیره دیتاست بالانس‌شده\n",
        "df_balanced = pd.DataFrame(X_balanced, columns=df.columns[:-1])\n",
        "df_balanced[target_column] = y_balanced\n",
        "\n",
        "balanced_file_path = \"/content/d6_balanced_withGRU.csv\"\n",
        "df_balanced.to_csv(balanced_file_path, index=False)\n",
        "print(f\"✅ Balanced dataset saved as {balanced_file_path}\")\n",
        "\n",
        "# 14. بررسی توزیع کلاس‌ها در پیش‌بینی مدل\n",
        "y_pred_probs = discriminator.predict(X_test, batch_size=512, verbose=1)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"🔹 توزیع کلاس‌ها در پیش‌بینی مدل:\", np.unique(y_pred, return_counts=True))\n",
        "\n",
        "# 15. محاسبه Accuracy و F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
        "print(f\"✅ F1-Score: {f1:.4f}\")\n"
      ]
    }
  ]
}